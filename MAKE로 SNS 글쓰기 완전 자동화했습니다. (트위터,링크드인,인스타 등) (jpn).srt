1
00:00:00,000 --> 00:00:06,800
MAKEを使ってSNS投稿を完全自動化しました。（ツイッター、リンクトイン、インスタなど）自動収集された記事をチェックするだけで、AIが各プラットフォームに合った文章を生成してくれます。

2
00:00:06,860 --> 00:00:09,200
さらに、投稿まで自動で行ってくれます。

3
00:00:09,420 --> 00:00:11,900
SNSの投稿が本当に簡単になりますよね？

4
00:00:12,140 --> 00:00:17,520
最近、SNSマーケティングはもはや選択ではなく必須だという話をよく耳にすると思います。

5
00:00:17,680 --> 00:00:25,740
実際、最近では普通の会社員でも、退勤後のSNS活動だけで本業の年収に匹敵するほどの収入を得ている事例がかなり多いんです。

6
00:00:25,741 --> 00:00:31,680
それだけ、セルフブランディングや将来の副業に活用する上で、SNSは本当に最高の手段だと思います。

7
00:00:31,940 --> 00:00:36,900
しかし、SNSの運営が決して簡単ではないことは、経験した方ならよくご存知でしょう。

8
00:00:37,280 --> 00:00:43,900
毎日新しいアイデアを思いつき、各プラットフォームの特性に合わせて投稿するのは、並大抵のことではありません。

9
00:00:44,350 --> 00:00:55,300
しかし、海外のマーケターたちは既にAI自動化システムを活用して、手を煩わせることなくSNSを運営し、大きな成果を上げているという事実をご存知ですか？

10
00:00:55,420 --> 00:00:55,720
はい。

11
00:00:55,740 --> 00:01:00,960
1日1投稿はもちろん、それ以上のコンテンツ発行も楽々とこなしている人が多いんです。

12
00:01:01,120 --> 00:01:09,680
そこで今日は、一度設定するだけでSNSへの投稿が自動化されるシステムを作る方法をご紹介したいと思います。

13
00:01:10,020 --> 00:01:18,920
このAI自動化システムを活用すれば、最新ニュースの収集から記事の作成、そして投稿まで、全てのプロセスを自動で処理できます。

14
00:01:19,340 --> 00:01:24,960
毎日何を書こうか悩んでいた日々から解放され、夢見ていた1日1投稿を実現できるでしょう。

15
00:01:24,961 --> 00:01:30,895
動画を最後までご覧いただければ、この自動化システムをそのまま適用できるように、テンプレートと詳細な

16
00:01:30,896 --> 00:01:36,620
プロンプト例、そして私がどのように実務に応用しているかまで、全てお伝えしますので、

17
00:01:36,640 --> 00:01:38,500
最後までご覧ください。

18
00:01:38,960 --> 00:01:47,740
ちなみに、この動画はリンクトインとツイッターを基準にお話しますが、インスタグラムや他のSNSチャンネルにも十分応用して適用できます。

19
00:01:49,740 --> 00:01:56,235
それでは、この自動化システムがどのように動作するのか、最終的な成果物を見ながら説明していきます。

20
00:01:56,582 --> 00:01:59,052
この自動化システムは大きく3段階で動作します。

21
00:01:59,433 --> 00:02:02,273
まず、1つ目はGoogleアラートを使って、

22
00:02:02,511 --> 00:02:06,620
様々なニュースサイトから、皆様が関心のあるテーマの最新ニュースを

23
00:02:06,791 --> 00:02:08,658
Googleスプレッドシートに自動で保存します。

24
00:02:09,138 --> 00:02:14,308
そして、収集されたニュースの中から、SNSに投稿したいニュースを選択して、

25
00:02:14,561 --> 00:02:19,060
チェックボックスを押すと、ChatGPTがそれを自動的にSNSの記事に変換してくれます。

26
00:02:19,380 --> 00:02:24,406
最後に、ChatGPTが作成した記事が気に入ったら、横の発行ボタンを押して

27
00:02:24,645 --> 00:02:27,593
実際のSNSに自動で投稿することも可能です。

28
00:02:27,813 --> 00:02:33,468
もちろん、このすべてのプロセスをチェックボックスを押す必要もなく、完全に自動で進行するように設定することもできます。

29
00:02:33,755 --> 00:02:36,361
この部分については後で詳しく説明する予定です。

30
00:02:36,827 --> 00:02:41,776
完全自動化する方法もありますが、まずこのチェックボックスを使う方法をお勧めする理由は、

31
00:02:41,981 --> 00:02:44,656
より自然な文章作成が可能だからです。

32
00:02:44,963 --> 00:02:50,312
プロンプトがある程度調整されるまでは、このようにチェックボックスで確認する段階を踏むことで、

33
00:02:50,690 --> 00:02:53,150
人が書いたような文章に仕上げることができます。

34
00:02:53,363 --> 00:02:57,866
私たちはこの自動化システムをmake.comというサイトを活用して作成する予定です。

35
00:02:58,206 --> 00:03:05,186
make.comを初めて使う方は、説明欄または固定コメントからmake.comに登録できますので、参考にしてください。

36
00:03:05,326 --> 00:03:11,206
それでは、SNS自動化の第一段階であるGoogleアラートの設定から始めましょう。

37
00:03:11,625 --> 00:03:16,412
このGoogleアラートを通じて、RSSで毎日ニュース記事を収集することができます。

38
00:03:16,739 --> 00:03:20,334
RSSという概念に少し馴染みがない方もいらっしゃるかもしれません。

39
00:03:20,787 --> 00:03:24,242
RSSとは、ユーザーがウェブサイトに直接アクセスしなくても、

40
00:03:24,501 --> 00:03:27,601
新しいコンテンツを受け取ることができるようにする技術です。

41
00:03:28,067 --> 00:03:32,767
多くのニュースサイトやブログが、独自にRSSフィードを提供しています。

42
00:03:33,107 --> 00:03:35,962
例えば、毎日経済のRSSを購読すれば、

43
00:03:36,227 --> 00:03:40,860
この毎日経済のサイトにアクセスしなくても、最新のニュースアラートを受け取ることができます。

44
00:03:41,207 --> 00:03:44,568
それぞれのニュースサイトからRSSを取得することもできますが、

45
00:03:44,839 --> 00:03:48,213
Googleアラートは、このような各サイトにアクセスする必要がなく、

46
00:03:48,466 --> 00:03:55,226
ここにキーワードを設定するだけで、関心のあるテーマに関するニュースを全て収集してくれるため、非常に便利です。

47
00:03:55,512 --> 00:04:00,453
こちらのGoogleアラートのサイトにアクセスして、お好みのキーワードを検索すると、

48
00:04:00,698 --> 00:04:03,585
そのキーワードに関連する最新ニュースが表示されます。

49
00:04:03,978 --> 00:04:06,572
私はAIで一度検索してみます。

50
00:04:07,225 --> 00:04:10,691
このように検索すると、AI関連のニュースがずらっと表示されます。

51
00:04:11,017 --> 00:04:15,584
これらのニュースを収集したい場合は、上部のアラートを作成をクリックしてください。

52
00:04:15,937 --> 00:04:18,390
そして、ここの鉛筆部分をクリックして、

53
00:04:18,477 --> 00:04:23,488
受信場所をRSSフィードに変更し、アラートを更新をクリックします。

54
00:04:24,135 --> 00:04:28,308
すると、このAIというキーワードの横に、この電波アイコンが表示されます。

55
00:04:28,682 --> 00:04:34,215
この電波アイコンをクリックすると、XMLで構成されたRSSニュースフィードのアドレスに移動します。

56
00:04:34,745 --> 00:04:39,381
このアドレスバーのアドレスを全選択した後、Ctrl Cを押してコピーしてください。

57
00:04:39,708 --> 00:04:43,961
いよいよmake.comを活用した自動化システムを構築していきます。

58
00:04:44,681 --> 00:04:48,161
まず、make.comにログインすると、私と同じ画面が表示されると思います。

59
00:04:48,215 --> 00:04:51,109
左側のシナリオボタンをクリックして、

60
00:04:51,439 --> 00:04:56,388
右上の「新しいシナリオを作成」をクリックして、新しいシナリオを作成してください。

61
00:04:57,941 --> 00:05:00,561
まず、ニュースを収集するシナリオを作成します。

62
00:05:00,775 --> 00:05:04,861
私のYouTubeの説明欄に、ある程度JSONファイルを作成しておきました。

63
00:05:05,088 --> 00:05:06,935
それをベースに始めてみましょう。

64
00:05:07,288 --> 00:05:11,010
まず、私のYouTubeの説明欄にあるJSONファイルをダウンロードしてください。

65
00:05:11,317 --> 00:05:15,172
ダウンロードされた方は、こちらのシナリオ下部の「...」をクリックして、

66
00:05:15,617 --> 00:05:17,828
「Blueprintをインポート」をクリックし、

67
00:05:18,393 --> 00:05:22,153
ダウンロードしたファイルをアップロードして、「保存」をクリックすると、

68
00:05:22,200 --> 00:05:24,166
私と同じような画面が表示されるはずです。

69
00:05:24,573 --> 00:05:28,266
ここで、RSSニュース収集の部分をクリックしてください。

70
00:05:28,546 --> 00:05:36,287
私と同じオプションウィンドウが表示されると思いますが、URL部分に、先ほど作成したフィードのアドレスをCtrl Vで貼り付けてください。

71
00:05:36,367 --> 00:05:39,027
これでAIニュースが収集されるはずです。

72
00:05:39,427 --> 00:05:45,111
Date FromとDate Toは、いつからいつまでのニュースを取得するかを入力する欄です。

73
00:05:45,533 --> 00:05:53,464
現在の場合は、「現在」と「-6」になっているので、現在から6日前までのニュースを取得することになっています。

74
00:05:53,884 --> 00:06:00,805
ここでは、6を1に変更して、今日から昨日までのニュースだけを取得してみましょう。

75
00:06:01,551 --> 00:06:05,771
そして、下を見ると、「Maximum Number of Returned Items」と書かれていますが、

76
00:06:05,884 --> 00:06:09,684
これは一度にいくつのニュースを取得するかを設定する部分です。

77
00:06:10,090 --> 00:06:13,817
今はニュースを5つまで取得するように設定されています。

78
00:06:14,257 --> 00:06:17,764
この設定のまま、「OK」をクリックして保存します。

79
00:06:18,564 --> 00:06:23,717
下部の「Run Once」ボタンをクリックすると、このシナリオが実行されます。一度クリックしてみましょう。

80
00:06:24,317 --> 00:06:26,542
「Run Anyway」をクリックしてください。

81
00:06:27,569 --> 00:06:32,529
クリックしてしばらく待つと、私のように上部に白い丸が表示されるはずです。

82
00:06:33,055 --> 00:06:36,590
この「RSSニュース収集」の上にある白い丸をクリックすると、

83
00:06:36,649 --> 00:06:40,185
実際にどのようなニュースが収集されたかを見ることができます。

84
00:06:40,738 --> 00:06:47,758
Bundle 1からBundle 2、Bundle 3と、全部で5つのニュースが取得されたことがわかります。

85
00:06:48,198 --> 00:06:52,682
各バンドルの中を見ると、ニュースのタイトル、そしてニュースの概要、

86
00:06:53,311 --> 00:06:55,938
URLも取得されていることがわかります。

87
00:06:56,078 --> 00:07:01,817
ここで重要なのは、今、ニュースのタイトルとURLだけで、ニュースの本文がないことです。

88
00:07:02,136 --> 00:07:06,952
本文を取得するには、このURLにアクセスして実際の記事内容を取得する必要がありますが、

89
00:07:07,263 --> 00:07:11,817
問題はこのURLが、実際の記事のURLではないということです。

90
00:07:12,443 --> 00:07:14,069
コピーして一度お見せします。

91
00:07:14,202 --> 00:07:19,157
これをコピーしてインターネットの窓に貼り付け、実際に移動するアドレスを確認すると、

92
00:07:19,629 --> 00:07:22,209
実際のアドレスはこちらであることが確認できます。

93
00:07:22,309 --> 00:07:26,430
そのため、このアドレスをこのようにきれいなアドレスに整理するために、

94
00:07:26,541 --> 00:07:30,495
このmake.comに3つのモジュールを事前に追加しておきました。

95
00:07:30,829 --> 00:07:37,031
それぞれのモジュールがフィルターのように機能し、複雑なURLを段階的にきれいに整理してくれます。

96
00:07:37,644 --> 00:07:41,384
最初のモジュールには、このようなパターンが入っています。

97
00:07:41,551 --> 00:07:44,317
これは正規表現という一種のルールです。

98
00:07:44,397 --> 00:07:47,838
この正規表現のルールは、今のURLから

99
00:07:48,310 --> 00:07:54,437
URL=の次の部分から&CTの間のURLだけを抽出するという意味です。

100
00:07:54,563 --> 00:07:59,623
そして、2つ目のモジュールは、この%3Dを=に置き換えるという意味です。

101
00:07:59,957 --> 00:08:07,376
そして、3つ目のモジュールは、このURLで%3Fという部分があれば、それを?に置き換えるという意味です。

102
00:08:08,042 --> 00:08:10,311
このように3回の精製作業を経ると、

103
00:08:10,628 --> 00:08:14,931
先ほどお見せしたように、きれいな記事URLに変わります。

104
00:08:15,497 --> 00:08:18,939
実際にこのURL精製3の白いモジュールをクリックして、

105
00:08:19,277 --> 00:08:23,024
このOperationの部分をクリックして、Outputの部分を確認してみると、

106
00:08:23,531 --> 00:08:27,535
このようにきれいに実際の記事URLが抽出されていることがわかります。

107
00:08:27,875 --> 00:08:31,535
さあ、このきれいになったURLで、実際のニュース本文を取得してみましょう。

108
00:08:31,908 --> 00:08:35,895
横のプラスボタンをクリックして、httpを検索してください。

109
00:08:36,127 --> 00:08:38,944
ここで、「Make a request」をクリックして追加してください。

110
00:08:39,024 --> 00:08:44,431
このURLの部分をクリックすると、ここに以前のモジュールが作成した結果がずらっと表示されます。

111
00:08:44,531 --> 00:08:48,869
この色で表示されたボックスが、一つのバスケットだと考えてください。

112
00:08:49,176 --> 00:08:53,109
それぞれのバスケットの中に、私たちが必要な情報が入っているんです。

113
00:08:53,269 --> 00:08:55,377
今、ここに入れるべき情報は、

114
00:08:55,451 --> 00:08:59,815
URL精製3番のモジュールを通してきれいにフィルターされたURLですよね。

115
00:09:00,269 --> 00:09:02,924
そのため、このURL精製3の下を見ると、

116
00:09:03,329 --> 00:09:07,589
このバスケットの中に何が入っているか、このようにプレビューで表示されます。

117
00:09:07,862 --> 00:09:10,203
ここにきれいに整えられたURLが入っているので、

118
00:09:10,281 --> 00:09:14,648
このURL精製3番のテキストバスケットをクリックして、このように追加すれば良いでしょう。

119
00:09:14,734 --> 00:09:20,693
メソッドは現在の状態のまま、getにしておくと、ニュースをget、つまり取得するという意味になります。

120
00:09:21,440 --> 00:09:23,947
ここで、ベテランだけが知っている裏技を一つお教えします。

121
00:09:24,413 --> 00:09:28,582
ほとんどのサイトは、自社のサイトから情報をスクレイピングされるのを防ぐために、

122
00:09:28,899 --> 00:09:32,905
このヘッダーを検査して、ロボットがスクレイピングしているようだと遮断するんです。

123
00:09:33,219 --> 00:09:38,422
そこで、私たちはこのサイトが本物の人間だと錯覚するように、偽のヘッダーを作成します。

124
00:09:38,695 --> 00:09:41,190
ヘッダー部分で、このように「ヘッダーを追加」をクリックして、

125
00:09:42,742 --> 00:09:45,634
name部分にcookieを追加してください。

126
00:09:45,994 --> 00:09:47,116
もう一度クリックして、

127
00:09:48,661 --> 00:09:50,634
「accept」と入力してください。

128
00:09:50,943 --> 00:09:54,203
そして、もう一度クリックして、今度は「accept」

129
00:09:56,221 --> 00:09:59,382
-Encoding」と入力して、

130
00:09:59,683 --> 00:10:05,539
もう一度追加して、「User-Agent」と追加します。

131
00:10:05,739 --> 00:10:09,390
これで、ウェブサイトのURLからニュースをスクレイピングできるようになります。

132
00:10:09,737 --> 00:10:12,943
実際にうまくスクレイピングできるかテストしてみましょう。

133
00:10:13,597 --> 00:10:14,770
「OK」をクリックしてください。

134
00:10:15,090 --> 00:10:17,530
下の「Run Once」をクリックしてテストしてみましょう。

135
00:10:18,337 --> 00:10:22,238
すると、このようにhttpモジュールの上に白い丸が表示され、

136
00:10:22,569 --> 00:10:28,022
クリックすると、5つのニュース、5つの結果が実行されたと表示されることがわかります。

137
00:10:28,422 --> 00:10:31,124
このうち、どれかのoperationをクリックすると、

138
00:10:31,449 --> 00:10:38,799
inputとしてこのような値が入り、このような値が出力されたと表示されることがわかります。

139
00:10:39,152 --> 00:10:44,106
ここで、outputの下にあるバンドルの下にあるデータをクリックすると、

140
00:10:45,464 --> 00:10:52,681
このように、該当のウェブページにあるニュース、記事を含むhtml全体がスクレイピングされたことがわかります。

141
00:10:53,041 --> 00:10:58,741
しかし、私たちにとって本当に必要なのは、このhtmlコード全体ではなく、ニュースの内容ですよね。

142
00:10:58,821 --> 00:11:02,634
そこで、このhtmlコードから実際のテキストだけを抽出する必要があります。

143
00:11:02,967 --> 00:11:05,427
このために、新しいモジュールを追加します。

144
00:11:05,907 --> 00:11:09,787
プラスボタンをクリックし、「Text parser」を検索してください。

145
00:11:10,107 --> 00:11:15,647
この「Text parser」をクリックすると、「html to text」というモジュールがあります。

146
00:11:16,560 --> 00:11:23,999
これを選択して、このhtmlの欄に変換したいhtmlを入れると、それをテキスト形式に変換してくれます。

147
00:11:24,086 --> 00:11:29,342
ここの空白をクリックすると、先ほどのように、以前のモジュールの結果の値がずらっと表示されます。

148
00:11:29,747 --> 00:11:36,022
ここで、私たちに必要なのは、このhttpモジュールの結果の値である、このデータの値を入れることです。

149
00:11:36,323 --> 00:11:39,574
データボタンをクリックして入れて、「OK」をクリックします。

150
00:11:39,940 --> 00:11:41,247
一度実行してみます。

151
00:11:41,834 --> 00:11:43,458
「Run Anyway」をクリックしてください。

152
00:11:44,964 --> 00:11:50,832
少し待つと、このようにhttpモジュールが実行され、この「Text parser」に5つの結果が生成されます。

153
00:11:51,231 --> 00:11:55,246
クリックすると、このOutputの中のテキスト欄を見ると、

154
00:11:55,510 --> 00:12:00,911
htmlが削除されたテキスト領域だけが残っていることがわかります。

155
00:12:01,011 --> 00:12:03,534
しかし、抽出されたニュースを詳しく見ると、

156
00:12:03,864 --> 00:12:08,174
ここで私たちが欲しいのは、この実際の本文ですが、本文だけでなく、

157
00:12:08,513 --> 00:12:15,930
このように共有やウェブサイトのメニュー部分など、このような部分もすべて一緒に含まれていることがわかります。

158
00:12:16,350 --> 00:12:22,566
ここで、私たちが必要なのは、このテキスト本文の部分だけなので、これだけをきれいに抽出してみましょう。

159
00:12:23,092 --> 00:12:27,793
その前に、追加したモジュールの名前も、もう少しわかりやすく変更しておきます。

160
00:12:27,933 --> 00:12:31,118
このhttpモジュールで右クリックして、

161
00:12:31,630 --> 00:12:35,530
「rename」をクリックして「ニュースデータ抽出」に変更しましょう。

162
00:12:35,816 --> 00:12:39,373
そして、「Text parser」も「rename」をクリックして、これは

163
00:12:39,713 --> 00:12:45,799
「htmlをテキストに変換」に変更します。

164
00:12:46,299 --> 00:12:50,820
次に、このモジュールの横にあるプラスボタンをクリックして、新しいモジュールを追加します。

165
00:12:51,293 --> 00:12:55,666
「Open AI」をクリックし、ここで「Create a compilation」をクリックしてください。

166
00:12:57,131 --> 00:13:03,325
初めてアクセスする方は、私のように接続されているのではなく、ここにボタン形式で表示されるはずです。

167
00:13:03,560 --> 00:13:07,439
接続方法は難しくないので、説明欄に詳しく残しておきます。

168
00:13:07,852 --> 00:13:11,349
それを使って接続を進めていただければ、私と同じウィンドウが表示されます。

169
00:13:12,409 --> 00:13:16,216
さあ、ニュース本文をきれいに抽出するための設定をしてみましょう。

170
00:13:17,230 --> 00:13:23,544
オプション部分のモデルで、GPT-3.5を選択します。

171
00:13:24,825 --> 00:13:28,585
3.5ターボ0125を選択します。

172
00:13:32,434 --> 00:13:36,420
モデルをGPT-4oにすると、精度は上がりますが、

173
00:13:36,492 --> 00:13:42,072
今ここに入っているテキストの量が、先ほどご覧いただいたように非常に多いですよね。

174
00:13:42,631 --> 00:13:46,408
そのため、GPT-4oにすると、費用が高くなりすぎてしまうので、

175
00:13:46,747 --> 00:13:49,953
ここでは安いGPT-3.5を使います。

176
00:13:50,300 --> 00:13:56,034
そして、メッセージの下にある「メッセージを追加」をクリックして、役割を「ユーザー」に設定してください。

177
00:13:56,534 --> 00:13:59,010
テキストコンテンツに、このように書いてください。

178
00:13:59,396 --> 00:14:06,176
「この文書から、このニュースと関係のない部分をすべて削除して。」

179
00:14:06,482 --> 00:14:11,324
「ニュースのタイトルとニュースの本文だけを残して。」

180
00:14:11,591 --> 00:14:12,191
#文書

181
00:14:13,744 --> 00:14:19,345
このようにして、ChatGPTにこのニュースのテーマが何かを知らせるために、ここに追加します。

182
00:14:19,825 --> 00:14:22,094
ニュースの横に括弧をつけた後、

183
00:14:22,490 --> 00:14:27,354
この中に、ニュースのタイトルが入っているこのRSS収集モジュール下部の

184
00:14:27,890 --> 00:14:30,703
「タイトル」部分をクリックして追加します。

185
00:14:31,003 --> 00:14:34,753
そして、文書の横にも、私たちが抽出するニュースの内容が入っている

186
00:14:34,977 --> 00:14:40,485
このhtmlテキスト変換モジュールの中にあるテキストをクリックして追加します。

187
00:14:40,851 --> 00:14:44,926
そして、この下にある「Show advanced settings」をオンにして、

188
00:14:45,566 --> 00:14:50,980
詳細メニューを開き、ここの温度を0.2程度に下げます。

189
00:14:51,289 --> 00:14:56,108
この温度の値が1に近いほど、創造的な回答が得られますが、

190
00:14:56,600 --> 00:15:03,130
今回はそれほど創造的な回答が必要なわけではないので、0に近い0.2程度に設定します。

191
00:15:03,670 --> 00:15:08,218
そして、Max completion tokenは、回答の長さを設定するものですが、

192
00:15:08,640 --> 00:15:13,027
ここでは回答の長さに制限を設けないために、0を入力します。

193
00:15:15,350 --> 00:15:19,907
「OK」をクリックした後、もう一度「Run Once」をクリックして実行してみましょう。

194
00:15:24,638 --> 00:15:29,113
費用がかかるため、最初のものだけ実行し、「停止」をクリックします。

195
00:15:32,675 --> 00:15:35,910
このように実行された後、「Open AI」上部のこの

196
00:15:36,247 --> 00:15:39,421
白い風船をクリックして、結果の値を確認すると、

197
00:15:39,640 --> 00:15:44,120
このResultの中に、実際のニュース本文がこのように抽出されていることがわかります。

198
00:15:44,628 --> 00:15:49,196
ちなみに、今は3.5、0125という一番費用の安いもので実行しているので、

199
00:15:49,536 --> 00:15:53,061
今、本文の抽出が非常に正確に行われているわけではありません。

200
00:15:53,440 --> 00:15:58,081
もう少し高いモデルを使うほど、この結果の値も非常に正確に出力されるので、

201
00:15:58,419 --> 00:16:02,460
その点は価格の部分を考慮して設定してください。

202
00:16:02,633 --> 00:16:08,581
このようにニュース本文がうまく出力されたので、この本文をGoogleスプレッドシートに保存します。

203
00:16:08,908 --> 00:16:10,727
まず、Googleスプレッドシートを作成しましょう。

204
00:16:10,879 --> 00:16:15,402
Googleスプレッドシートのサイトにアクセスして、空白のスプレッドシートをクリックして作成した後、

205
00:16:16,302 --> 00:16:23,287
「ニューススクラップ」という名前をつけて、一番上の行に「タイトル」「原文URL」

206
00:16:23,985 --> 00:16:27,133
「作成日」とヘッダーを追加します。

207
00:16:27,593 --> 00:16:29,566
再びmake.comに戻り、

208
00:16:30,024 --> 00:16:36,212
この最後のOpen AIモジュールの右側にあるプラスボタンをクリックして、このGoogleスプレッドシートをクリックします。

209
00:16:36,879 --> 00:16:38,933
そして、「行を追加」を選択してください。

210
00:16:40,021 --> 00:16:44,191
私はすでにGoogleスプレッドシートと連携しているので、このように表示されていますが、

211
00:16:44,482 --> 00:16:46,784
皆さんにはボタンが表示されると思います。

212
00:16:46,937 --> 00:16:49,721
それをクリックしてログインすると接続されます。

213
00:16:50,215 --> 00:16:55,316
接続した後、このスプレッドシートIDの下にあるボタンをクリックして、

214
00:16:56,121 --> 00:17:00,107
先ほど作成したニューススクラップを検索して追加します。

215
00:17:00,454 --> 00:17:04,774
そして、シート名はsheet1、先ほどヘッダーを作成したので、

216
00:17:05,192 --> 00:17:07,771
ヘッダーの有無は「はい」にしてください。

217
00:17:08,138 --> 00:17:13,383
そして、この「値」という部分が、Googleスプレッドシートに入力するデータを入力する部分です。

218
00:17:13,756 --> 00:17:18,301
タイトル列に、ニュースのタイトルを入力する必要があるので、このポップアップウィンドウで、

219
00:17:18,740 --> 00:17:24,116
このRSSニュース収集モジュールの下にある「title」をクリックして入力します。

220
00:17:24,710 --> 00:17:28,241
次に、原文の部分には、ニュース記事の原文を入力する必要があるので、

221
00:17:28,886 --> 00:17:35,407
このポップアップウィンドウで、Open AIモジュールを使って抽出したニュースの原文を追加します。

222
00:17:35,561 --> 00:17:37,987
この「result」の値を入力すればOKです。

223
00:17:38,358 --> 00:17:39,746
そして、URL欄には、

224
00:17:39,824 --> 00:17:46,958
このURL精製3番のモジュールを通過して、きれいに整理されたURLの結果の値テキストを入力します。

225
00:17:47,351 --> 00:17:52,960
そして、作成日の部分には、このRSSニュース収集モジュール下部にある、「date created」

226
00:17:53,440 --> 00:17:56,514
作成された日付を選択して入力すればOKです。

227
00:17:56,934 --> 00:17:58,165
「OK」をクリックしてください。

228
00:17:58,725 --> 00:18:02,572
実際にうまく動作するか、「Run Once」をクリックしてみましょう。

229
00:18:05,140 --> 00:18:08,312
クリックして実行されるまで、少し待ってみましょう。

230
00:18:08,372 --> 00:18:12,576
このように実行中に、エラーが表示されることがあります。

231
00:18:12,690 --> 00:18:18,718
今このエラーが表示された理由は、GPT3.5の最大トークン数を超えたためです。

232
00:18:19,032 --> 00:18:22,038
このような場合は、ここで右クリックして、

233
00:18:22,345 --> 00:18:25,284
「エラーハンドラーを追加」をクリックしてください。

234
00:18:25,771 --> 00:18:29,504
このエラーが発生した場合にどのように処理するかを記述することができます。

235
00:18:30,004 --> 00:18:34,123
このエラーハンドラーモジュールの中から「ignore」を選択すると、

236
00:18:34,528 --> 00:18:37,515
そのエラーを無視して実行を継続します。

237
00:18:37,730 --> 00:18:43,797
4つまでは、結果の値がすべて実行されたと表示されますが、ニューススクラップのGoogleシートに移動してみましょう。

238
00:18:44,377 --> 00:18:48,238
すると、このように4つのニュースがうまくスクラップされていることがわかります。

239
00:18:48,344 --> 00:18:54,186
さて、エラーハンドラーを追加したので、もう一度実行して、うまく実行されるかどうか確認してみましょう。

240
00:18:54,552 --> 00:18:57,699
「Run Once」をクリックすると、このように実行されています。

241
00:18:58,139 --> 00:19:01,340
1回目の実行が完了し、2回目の実行も完了しました。

242
00:19:01,747 --> 00:19:04,217
3回目の実行が完了したと表示されています。

243
00:19:04,537 --> 00:19:06,910
4回目もこのようにうまく処理され、

244
00:19:06,934 --> 00:19:14,686
エラーが発生した1件は無視し、残りの4件はこのニューススクラップにうまく保存されていることがわかります。

245
00:19:17,250 --> 00:19:20,063
このように、ニュースを収集するフローを作成しました。

246
00:19:20,436 --> 00:19:22,804
ここから、もう一つだけ修正すれば完了です。

247
00:19:23,224 --> 00:19:25,477
それは、このシナリオの実行間隔です。

248
00:19:25,683 --> 00:19:29,273
ここの最初のモジュールの左側にある時計アイコンをクリックして、

249
00:19:29,718 --> 00:19:32,699
ここでシナリオの実行間隔を設定することができます。

250
00:19:32,872 --> 00:19:35,678
デフォルト値は、ご覧のとおり15分になっています。

251
00:19:36,072 --> 00:19:39,951
ここで、実行間隔を希望の間隔に変更できます。

252
00:19:40,171 --> 00:19:42,840
毎朝ニュースを収集したい場合は、

253
00:19:43,506 --> 00:19:48,483
ここにある「シナリオを実行」の下をクリックして、実行間隔を「Every Day」に変更し、

254
00:19:48,883 --> 00:19:53,336
時間を...希望の時間に変更した後、「OK」をクリックします。

255
00:19:53,829 --> 00:19:57,408
すると、「シナリオを有効にしますか？」と表示されます。

256
00:19:57,728 --> 00:20:00,580
「有効にする」つまり「Activate Scenario」をクリックすると、

257
00:20:00,898 --> 00:20:07,777
この部分がオンになり、毎日午後4時6分に実行するように設定が完了します。

258
00:20:08,758 --> 00:20:13,830
さあ、この収集したニュースをもとに、SNS自動発行システムを作っていきましょう。

259
00:20:14,216 --> 00:20:18,755
まず、Googleスプレッドシートに「SNS記事に変換」というヘッダーを追加します。

260
00:20:19,987 --> 00:20:22,718
そして、その下にチェックボックスを追加します。

261
00:20:25,104 --> 00:20:29,478
そして、この横に「リンクトイン」「ツイッター」と続けて記述します。

262
00:20:33,066 --> 00:20:39,514
この「SNS記事に変換」のチェックボックスがチェックされたら、横にリンクトイン用の記事とツイッター用の記事が生成されるようにします。

263
00:20:40,034 --> 00:20:42,537
そして、横に「発行」と記述します。

264
00:20:43,303 --> 00:20:45,309
ここにもチェックボックスを追加します。

265
00:20:47,577 --> 00:20:52,083
このチェックボックスにチェックが入ると、ここに作成された記事がすぐに発行されるようにします。

266
00:20:52,496 --> 00:20:57,312
まず、このチェックボックスにチェックが入ったときに、Makeで感知できるようにする必要があります。

267
00:20:57,739 --> 00:21:00,622
その役割を果たす連結点をウェブフックといいます。

268
00:21:01,062 --> 00:21:05,560
ウェブフックは、make.comのGoogleスプレッドシート拡張機能を利用すれば設定できます。

269
00:21:06,170 --> 00:21:10,054
拡張機能メニューから「アドオン」、「アドオンを取得」をクリックしてください。

270
00:21:11,595 --> 00:21:13,738
ここで「make」を検索すると、

271
00:21:15,310 --> 00:21:16,942
このように拡張機能が表示されます。

272
00:21:17,118 --> 00:21:19,668
私はすでにインストールされているので「削除」と表示されますが、

273
00:21:19,909 --> 00:21:23,382
皆さんには「インストール」と表示されると思います。「インストール」をクリックしてインストールを進めてください。

274
00:21:24,300 --> 00:21:30,401
そして、make.comに戻り、再び「新しいシナリオを作成」をクリックして新しいシナリオを作成してください。

275
00:21:32,320 --> 00:21:38,080
ここで、Googleスプレッドシート、「変更を監視」モジュールをクリックして追加してください。

276
00:21:38,220 --> 00:21:43,260
このウェブフックの部分が表示されたら、横の「追加」をクリックして新しいウェブフックを作成します。

277
00:21:45,069 --> 00:21:47,260
すると、このようにウェブフックURLが表示されます。

278
00:21:47,486 --> 00:21:52,335
これを「アドレスをコピー」をクリックしてコピーし、再びGoogleスプレッドシートに戻って、

279
00:21:52,799 --> 00:21:58,932
「拡張機能」、「make for Google sheets」、「設定」をクリックして、横のサイドバーを表示します。

280
00:21:59,200 --> 00:22:01,780
すると、ここにウェブフックURLを入力する欄があります。

281
00:22:02,000 --> 00:22:07,000
ここにCtrl vで貼り付けて「保存」をクリックすると接続されます。

282
00:22:09,240 --> 00:22:11,087
このように設定が完了したと表示されます。

283
00:22:11,220 --> 00:22:14,040
makeシナリオに戻って「OK」をクリックします。

284
00:22:14,146 --> 00:22:17,312
テストのために、下の「Run Once」をクリックします。

285
00:22:17,412 --> 00:22:21,147
すると、このように丸が回転して、Googleスプレッドシートを感知していると表示されます。

286
00:22:21,373 --> 00:22:24,881
この状態で、Googleスプレッドシートでチェックボックスをこのようにチェックすると、

287
00:22:26,876 --> 00:22:29,824
このように感知されたと表示され、結果の値が表示されます。

288
00:22:29,924 --> 00:22:33,238
ここで、「Output」バンドルの中身を確認すると、

289
00:22:33,497 --> 00:22:37,228
「old value」は「false」つまりチェックボックスがチェックされていなかったのですが、

290
00:22:37,252 --> 00:22:43,140
今の「value」は「true」でチェックボックスの変更をmakeがうまく感知していることがわかります。

291
00:22:43,540 --> 00:22:46,013
そして、「range」部分をクリックして開くと、

292
00:22:46,500 --> 00:22:52,312
この感知された事項が2行目と5列目で感知されたと表示されます。

293
00:22:52,585 --> 00:22:59,020
実際に私たちがチェックしたこのチェックボックスの位置を見ると、2行目と5列目で間違いないですよね？

294
00:22:59,740 --> 00:23:02,260
このように、うまく感知していることが確認できます。

295
00:23:02,706 --> 00:23:06,720
ウェブフックが正常に動作することを確認したので、引き続き作成してみましょう。

296
00:23:07,180 --> 00:23:12,700
モジュールの横のプラスボタンをクリックして、「フロー制御」の中にある「ルーター」を追加してください。

297
00:23:13,154 --> 00:23:15,500
追加すると、このように道が2つに分かれます。

298
00:23:15,940 --> 00:23:20,399
上側はSNS記事変換チェックボックスをチェックすると記事が書かれるフロー、

299
00:23:20,525 --> 00:23:25,660
下側は発行チェックボックスをチェックすると記事が発行されるフローを作成します。

300
00:23:26,131 --> 00:23:32,020
この上側の線の上で右クリックし、「フィルター設定」をクリックして、このフィルターウィンドウを表示してください。

301
00:23:32,240 --> 00:23:36,584
SNS記事変換チェックボックスをクリックしたときに、ここが実行される必要があるので、

302
00:23:36,815 --> 00:23:45,432
古い値が「false」で、ここの値が「true」というルールを作成します。

303
00:23:45,559 --> 00:23:50,328
そして、「end」をもう一つ追加して、「Column start」をクリックします。

304
00:23:50,421 --> 00:23:55,540
今、設定しようとしているチェックボックスの位置は、1、2、3、4、5番目の列ですよね？

305
00:23:55,680 --> 00:23:59,051
そのため、「Column start」部分に5を入力し、

306
00:23:59,075 --> 00:24:04,200
もう一つ「end」を追加して、「Column end」も同じく5と追加します。

307
00:24:04,580 --> 00:24:06,031
「OK」をクリックしてください。

308
00:24:06,131 --> 00:24:11,080
これで、SNS記事変換チェックボックスにチェックを入れた時だけ、このフローが実行されるようになります。

309
00:24:11,742 --> 00:24:17,602
その後、この後ろにプラスボタンをクリックして、「Open AI Create a Completion」を選択します。

310
00:24:17,695 --> 00:24:24,784
モデルをGPT-4o Miniに選択します。

311
00:24:24,977 --> 00:24:28,555
そして、「メッセージを追加」をクリックして、役割を「ユーザー」に設定します。

312
00:24:29,342 --> 00:24:34,660
このテキストコンテンツの部分に、YouTubeの説明欄にあるプロンプトをコピーして貼り付けます。

313
00:24:35,455 --> 00:24:42,140
このプロンプトは、ChatGPTにマーケターの役割を担わせ、リンクトインの記事とツイッターの記事をうまく書いてくれるように依頼するものです。

314
00:24:42,540 --> 00:24:47,307
ChatGPTに依頼する際、SNS記事に変換する記事の原文を入力する必要があるので、

315
00:24:47,965 --> 00:24:52,276
ここにある「読んで分析してください。」という次の部分に記事の原文を追加します。

316
00:24:52,589 --> 00:24:58,160
このGoogleスプレッドシートで、記事の原文がどこにあるかを確認すると、2列目にあることがわかります。

317
00:24:58,666 --> 00:25:04,920
そのため、このポップアップウィンドウで、「Row Value」の中にある2列目のB列をクリックして追加します。

318
00:25:05,240 --> 00:25:10,079
そして、出所も一緒に表記するために、URLもこの原文リンクの横の部分に追加します。

319
00:25:10,300 --> 00:25:15,857
Googleスプレッドシートで、URLがどこにあるかを確認すると、3列目にあることがわかります。

320
00:25:16,843 --> 00:25:22,161
そのため、ここのプロンプト部分で、ポップアップウィンドウから3列目のC列を入力します。

321
00:25:23,760 --> 00:25:28,758
そして、下にスクロールして、このMax Tokensを1000程度に設定します。

322
00:25:28,938 --> 00:25:32,739
そして、この下にある「Show Advanced Settings」をオンにした後、

323
00:25:33,470 --> 00:25:36,724
この温度を0.32程度に設定します。

324
00:25:37,694 --> 00:25:41,015
残りはデフォルト値のままにして、「OK」をクリックしてください。

325
00:25:41,128 --> 00:25:43,280
「Run Once」をクリックしてテストしてみましょう。

326
00:25:43,547 --> 00:25:45,841
Googleシートに移動した後、

327
00:25:46,406 --> 00:25:51,747
文章を書きたいニュースの横のチェックボックスをオンにして、再びMakeシナリオに戻ると、

328
00:25:52,007 --> 00:25:58,940
このように変化を感知し、ChatGPTにInputが入り、このように文章を書いてくれることがわかります。

329
00:25:59,349 --> 00:26:00,954
結果の値を確認すると、

330
00:26:01,326 --> 00:26:05,401
このOutputの部分のResultをクリックすると、このようにリンクトインの記事と

331
00:26:05,840 --> 00:26:08,267
ツイッターの記事がうまく書かれていることが確認できます。

332
00:26:08,360 --> 00:26:11,769
ここで、リンクトインの記事部分は、Googleスプレッドシートのリンクトインの列に

333
00:26:12,080 --> 00:26:16,307
そして、ツイッター部分は、このGoogleスプレッドシートのツイッターの列部分に追加します。

334
00:26:16,501 --> 00:26:22,480
今は、2つの記事が一つにまとまっているので、分離するために正規表現というツールを使用します。

335
00:26:22,892 --> 00:26:24,999
正規表現は先ほども述べたように、

336
00:26:25,091 --> 00:26:29,812
テキストから、私たちが欲しい部分だけを抽出する一種の数式だと考えてください。

337
00:26:30,006 --> 00:26:32,516
ここにあるOpen AIの横のプラスボタンをクリックして、

338
00:26:33,619 --> 00:26:38,226
Text parserの下にある「一致パターン」を選択します。

339
00:26:39,340 --> 00:26:45,820
そしてここに、私がYouTubeの固定コメントに掲載しておいた正規表現パターンをコピーして貼り付けます。

340
00:26:45,946 --> 00:26:48,818
そして、「グローバルマッチ」のみ「はい」に変更すると、

341
00:26:49,151 --> 00:26:53,747
テキストを最初から最後まで検索し、このパターンと一致するすべての値を探してくれます。

342
00:26:54,066 --> 00:27:00,460
そして、「テキスト」の部分に、先ほどChatGPTが作成した「Result」の結果の値を入力します。

343
00:27:00,689 --> 00:27:04,940
「OK」をクリックし、もう一度テストのために「Run Once」をクリックします。

344
00:27:05,360 --> 00:27:07,020
「Run Anyway」をクリックしてください。

345
00:27:07,980 --> 00:27:14,220
そしてGoogleスプレッドシートに戻り、文章を書きたいニュースの横の部分にチェックボックスを入れます。

346
00:27:15,040 --> 00:27:18,268
そして、このように戻ってくると、Open AIが文章を作成し、

347
00:27:18,686 --> 00:27:21,827
この文章をText parserが2つに分割して、このように

348
00:27:21,858 --> 00:27:26,007
リンクトインの記事とツイッターの記事に分離してくれたことが確認できます。

349
00:27:26,180 --> 00:27:28,784
次に、これらの記事をGoogleスプレッドシートに保存します。

350
00:27:29,674 --> 00:27:35,700
「Text parser」の横にあるプラスボタンをクリックし、「フロー制御」を選択します。

351
00:27:36,108 --> 00:27:41,018
上側にはリンクトインの投稿、下側にはツイッターの投稿が保存されるようにフィルターをかけます。

352
00:27:41,580 --> 00:27:44,269
先ほどのこの「Text parser」の結果を見ると、

353
00:27:45,046 --> 00:27:51,880
最初のリンクトインの記事は、iが1と表示され、2番目のツイッターの記事はiが2と表示されています。

354
00:27:52,040 --> 00:27:54,240
これを基準にフィルターをかけます。

355
00:27:54,360 --> 00:28:00,900
ここの最初の線をクリックしてフィルターをオンにし、ポップアップから「i」をクリックして「1」と入力します。

356
00:28:01,080 --> 00:28:02,360
「OK」をクリックしてください。

357
00:28:02,460 --> 00:28:06,640
これで、最初の記事であるリンクトインの記事だけがこちらを通るようになります。

358
00:28:06,772 --> 00:28:09,591
そして、プラスボタンをクリックし、今度はGoogleスプレッドシート、

359
00:28:10,249 --> 00:28:16,185
「行を更新」を選択し、このスプレッドシートIDに、先ほど作成した

360
00:28:18,125 --> 00:28:20,359
「ニューススクラップ」シートを選択し、

361
00:28:22,131 --> 00:28:28,283
「シート名」：sheet1、そして「行番号」はここで「row end」を選択します。

362
00:28:28,377 --> 00:28:33,846
そして、「値」の部分に、今、リンクトインの記事を保存しているので、このリンクトイン列に

363
00:28:34,253 --> 00:28:37,060
この「fallback match」の部分を入力すればOKです。

364
00:28:37,522 --> 00:28:38,960
「OK」をクリックしてください。

365
00:28:39,395 --> 00:28:46,740
2つ目も同様にフィルターを「i」で入力し、そして、値は「2」と入力した後、「OK」をクリックします。

366
00:28:47,640 --> 00:28:51,026
同じように、Googleスプレッドシートの「行を更新」を追加し、

367
00:28:51,910 --> 00:28:55,382
残りのオプションは上のものと全く同じに設定します。

368
00:28:55,509 --> 00:29:01,120
そして、「値」の部分だけ、リンクトインの部分ではなくツイッターの部分に「fallback match」を入力すればOKです。

369
00:29:02,180 --> 00:29:09,160
もう一度、正常に動作するかどうか確認するために、「Run Once」をクリックして、Googleスプレッドシートでチェックボックスをオンにします。

370
00:29:10,260 --> 00:29:16,880
すると、このように感知され、ChatGPTが記事を作成して、Googleスプレッドシートが実行されたと表示されます。

371
00:29:17,120 --> 00:29:23,032
シートに移動すると、このようにリンクトインの記事とツイッターの記事がうまく生成されていることが確認できます。

372
00:29:23,152 --> 00:29:27,460
記事の自動生成部分が完成したので、記事の自動発行部分も作成してみましょう。

373
00:29:28,140 --> 00:29:31,647
この最初のルーターの下の線部分にフィルターを作成します。

374
00:29:31,860 --> 00:29:35,634
今回は、発行チェックボックスにチェックを入れたときに、ここが実行される必要があるため、

375
00:29:35,732 --> 00:29:41,085
「Column start」を発行チェックボックスがある列である8列目に入力します。

376
00:29:41,992 --> 00:29:45,579
そして、「end rule」を追加した後、「Column end」も8番目

377
00:29:46,252 --> 00:29:55,786
そして、「Old value」が「False」で、「value」の値が「True」の場合に設定します。

378
00:29:56,000 --> 00:30:00,555
そして、ここのプラスボタンをクリックして、「フロー制御」の「ルーター」を追加してください。

379
00:30:00,695 --> 00:30:04,565
そして、最初のプラスボタンをクリックして「リンクトイン」を検索し、

380
00:30:04,993 --> 00:30:08,558
ここで「Create a user text post」をクリックして追加します。

381
00:30:08,698 --> 00:30:14,900
私は事前に接続しているので、このように表示されますが、皆さんはここにあるボタンをクリックして接続を進めてください。

382
00:30:15,080 --> 00:30:19,060
そして、「コンテンツ」の部分にリンクトインの投稿記事を入力します。

383
00:30:19,490 --> 00:30:25,422
Googleスプレッドシートで、リンクトインの投稿が入っている列であるF列を選択すれば良いので、

384
00:30:25,720 --> 00:30:28,900
この「row value」の中にあるF列を入力します。

385
00:30:30,110 --> 00:30:32,720
「OK」をクリックすると、設定は完了です。

386
00:30:33,055 --> 00:30:42,960
この部分も同様にX、つまりツイッターを検索して、このXの中にある「Create post」をクリックします。

387
00:30:43,102 --> 00:30:45,938
私は事前に連携しているので、このように表示されますが、

388
00:30:46,251 --> 00:30:52,712
ツイッターの場合は、事前にAPI申請が承認されないと、Makeでこのように投稿することはできません。

389
00:30:52,852 --> 00:30:59,197
面倒ではありますが、このAPI申請は難しくないので、この申請方法をリンクで残しておきます。

390
00:30:59,662 --> 00:31:04,664
それを参考にして、ツイッターの連携を完了してから、このテキストコンテンツの部分に、

391
00:31:05,089 --> 00:31:09,280
ツイッターの内容が入っているこのG列を入力します。

392
00:31:09,780 --> 00:31:15,944
「row values」の中にあるG列をクリックして入力し、「OK」をクリックすると設定は完了です。

393
00:31:16,971 --> 00:31:19,174
では、自動発行をテストしてみましょう。

394
00:31:19,420 --> 00:31:23,006
「Run Once」をクリックして「Way for New Data」をクリックした後、

395
00:31:23,584 --> 00:31:27,388
Googleスプレッドシートで、発行したい記事の横の発行チェックボックスをオンにして、

396
00:31:27,726 --> 00:31:31,680
Makeシナリオに戻ると、このように実行されていることがわかります。

397
00:31:31,930 --> 00:31:34,630
実際にリンクトインにアクセスして確認してみましょう。

398
00:31:38,260 --> 00:31:42,560
このようにリンクトインの記事がうまく投稿されていることが確認できます。

399
00:31:43,159 --> 00:31:47,240
ツイッターもこのように記事がうまく生成されていることが確認できます。

400
00:31:47,553 --> 00:31:49,652
次に、この下のスケジューリング部分にある、

401
00:31:49,772 --> 00:31:56,967
このGoogleスプレッドシートを常に感知するように、「Immediately as Data Arrives」をオンにするとシステム設定が完了します。

402
00:31:57,566 --> 00:32:03,896
既存のデータを削除するために、「Delete Old Data」、「Delete」をクリックすると設定が完了します。

403
00:32:03,949 --> 00:32:06,052
これからは、チェックボックスを一つクリックするだけで、

404
00:32:06,351 --> 00:32:12,820
AIが自動的に文章を作成し、発行までしてくれるので、夢に描いていた1日1投稿も現実のものにすることができるでしょう。

405
00:32:13,140 --> 00:32:20,300
ちなみに、私はこのシステムで、自分の関心分野の最新ニュースを自動で収集し、SNSに投稿しています。

406
00:32:20,767 --> 00:32:25,660
特に、複数のSNSを通じてセルフブランディングをしようとしている方には、強くお勧めします。

407
00:32:26,139 --> 00:32:30,665
そして、AIが書いた記事は大丈夫なのかと心配する人もいるかと思います。

408
00:32:30,880 --> 00:32:37,140
このシステムは、すべての記事がまずGoogleスプレッドシートに保存されるため、発行前に確認して修正できます。

409
00:32:37,500 --> 00:32:40,532
結果を見ながらプロンプトを少しずつ改善し、

410
00:32:40,764 --> 00:32:45,880
この生成された記事の品質が十分に満足できるようになったら、この発行段階も自動化できます。

411
00:32:46,185 --> 00:32:47,380
方法は簡単です。

412
00:32:47,558 --> 00:32:55,720
このMakeシナリオで、この発行部分のリンクトインとXモジュールを、この更新モジュールの下に取り付ければOKです。

413
00:32:56,333 --> 00:33:03,320
そうすると、ニュースを選択するだけで、記事作成から発行まで一度に処理されるワンステップ自動化にアップグレードできます。

414
00:33:03,761 --> 00:33:08,316
そして、ここまでご覧いただいた方のために、このシステムをすぐに活用できるように、

415
00:33:08,661 --> 00:33:13,080
make.comシナリオテンプレートとプロンプト例をすべて無料で提供しています。

416
00:33:13,455 --> 00:33:22,912
必要な方は、固定コメントから私のグループチャットコミュニティまたはニュースレターを購読していただければ、すぐにダウンロードできますので、参考にしてください。

417
00:33:23,530 --> 00:33:27,868
グループチャットでは、様々なAI活用チップに関する質問や回答を交換することもできるので、

418
00:33:28,255 --> 00:33:33,560
このような自動化システムの構築に困難を感じている方も、お気軽にご参加いただければと思います。

419
00:33:33,940 --> 00:33:40,580
今日はこのように、make.comを活用してSNS投稿を自動化するシステムを構築する方法について説明しました。

420
00:33:40,880 --> 00:33:45,580
より多くのAI活用チップについて知りたい場合は、チャンネル登録と通知設定をお願いします。

421
00:33:45,935 --> 00:33:49,633
次の動画では、さらに有益な情報と共にお会いしましょう。

422
00:33:49,693 --> 00:33:51,220
ご視聴ありがとうございました。